{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polder pumping station - machine learning tutorial\n",
    "## Polder pumping station\n",
    "Because a polder closely resembles a bath tub, it will slowly fill with water because of rainfall if it's not emptied by a pump. \n",
    "\n",
    "![Polder](https://raw.githubusercontent.com/SGnoddeHHDelfland/PolderGemaalBesturing/main/images/polder1.png)\n",
    "\n",
    "This is done by a polder pumping station - or _poldergemaal_ in Dutch. \n",
    "This is a small building with a big pump that can remove water from the polder to canals that eventually transfer it to sea.\n",
    "\n",
    "\n",
    "If the polder is emptied too much, the land in the polder has too little water for nature and farmers. Therefore, an ideal water level is determined by the water board. The pumping station is managed in such way, that this level is followed as closely as possible, by turning the pumps on and off. This resembles the working of for example an oven or central heating in a house.\n",
    "\n",
    "_This is what a polder pumping station looks like, although it comes in many shapes and sizes._\n",
    "\n",
    "![Gemaal 1](https://raw.githubusercontent.com/SGnoddeHHDelfland/PolderGemaalBesturing/main/images/gemaal1.jpg)\n",
    "\n",
    "_Schematic pumping station._\n",
    "\n",
    "![Gemaal 2](https://raw.githubusercontent.com/SGnoddeHHDelfland/PolderGemaalBesturing/main/images/gemaal2.png)\n",
    "\n",
    "In this tutorial, you will create a control system which tries to follow this ideal water level as closely as possible, while it's raining in the polder. This rain is generated randomly. The water level we're aiming at is 0.5 m. \n",
    "\n",
    "\n",
    "## (Deep) Reinforcement learning\n",
    "We'll be using a technique called deep reinforcement learning. Based on simulations, the method tries to decide what is the best action it can do. It uses so-called neural networks, which are originally based on the workings of human brains. These kinds of models have been used to create the best chess robots in the world and for the first time beat the best human in the game of Go. \n",
    "\n",
    "_A Neural Network_\n",
    "\n",
    "![nn](https://raw.githubusercontent.com/SGnoddeHHDelfland/PolderGemaalBesturing/main/images/nn.png)\n",
    "\n",
    "\n",
    "The model consists of a number of parts: \n",
    "- Environment: the world the agent interacts with\n",
    "- Agent: the model that alters the environment\n",
    "- State: the state the world (environment) is in at a moment\n",
    "- Reward: this is important; in order to learn to do what we want, the system should deliver rewards. These should be high when the model does what we want it to do, and low when it's doing things that we don't want it to do.\n",
    "\n",
    "![reinforcement learning](https://raw.githubusercontent.com/SGnoddeHHDelfland/PolderGemaalBesturing/main/images/reinforcementlearning.png)\n",
    "\n",
    "## Reward\n",
    "In this tutorial, the first question you will try to solve, is the reward function. You will try to make a function that is high when the water level is around the level we want (so 0.5 m) and low when the water level is away from that number. In this example, we'll flip this and make a penalty: the further away from 0.5, the higher the penalty and the more negative the reward becomes. \n",
    "\n",
    "## Python\n",
    "The programming is done in the programming called Python. This is a relatively easy programming language and is widely used by data scientist.\n",
    "We're using a so-called Notebook to run the Python code in the browser. Press the play button or press `shift`+`enter` to run a cell. You can basically run all cells after one another, but there's one cell which you'll have to alter before running.\n",
    "\n",
    "## Hints\n",
    "There is a number of hints near the first question. Run a cell with the hint to see the hint.\n",
    "\n",
    "Below is the first hint, but first run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "path_json = r\"https://raw.githubusercontent.com/SGnoddeHHDelfland/PolderGemaalBesturing/main/hints.json\"\n",
    "response = requests.get(path_json)\n",
    "hints = response.json()\n",
    "def print_hints(number):\n",
    "    print(hints['hints'][number])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be your first hint, which we've basically already talked about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hints(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to install the model we're using. This is installed on Google, not on your pc. Don't mind all the text that will show up below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IvA-yUcDXMt4",
    "outputId": "a44e0208-9cc1-4b23-ab2e-f0812d571841"
   },
   "outputs": [],
   "source": [
    "!pip install tensorforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbcWyaR4XuRA"
   },
   "outputs": [],
   "source": [
    "from tensorforce.environments import Environment\n",
    "from tensorforce.agents import Agent\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Create a penalty function behind the `penalty = `. This should be a function of the water level (`water_level`).\n",
    "\n",
    "Note: if you want to use a power (tot de macht), this is written as `**` in Python, instead of `^` for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_penalty(water_level):\n",
    "    penalty = ...\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hints(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hints(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hints(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQCzZVF0XMt_"
   },
   "outputs": [],
   "source": [
    "class PolderEnvironment(Environment):\n",
    "    \"\"\"Simple polder environment. It is a polder with a single pump attached to it. \n",
    "    If the pump is activated, the waterlevel in the polder will lower. \n",
    "    Rainfall will cause the waterlevel in the polder to rise. \n",
    "    The amount of rainfall is random. \n",
    "    The goal will be to keep the waterlevel between 0.0 and 1.0m NAP and to keep this up for at least 100 timesteps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.water_level = np.random.uniform(low=0.0, high=1.0, size=(1,))\n",
    "\n",
    "    def states(self):\n",
    "        return dict(type='float', shape=(1,),  min_value=0.0, max_value=1.0)\n",
    "\n",
    "    def actions(self):\n",
    "        return dict(type='int', num_values=2)\n",
    "\n",
    "    # Optional, should only be defined if environment has a natural maximum\n",
    "    # episode length\n",
    "    def max_episode_timesteps(self):\n",
    "        return 500\n",
    "\n",
    "\n",
    "    # Optional\n",
    "    def close(self):\n",
    "        super().close()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset state.\"\"\"\n",
    "        self.timestep = 0\n",
    "        self.water_level = np.random.uniform(low=0.0, high=1.0, size=(1,))\n",
    "        return self.water_level\n",
    "\n",
    "\n",
    "    def response(self, action):\n",
    "        \"\"\"Respond to an action.\"\"\"\n",
    "        return self.water_level - (action * 0.2)\n",
    "\n",
    "\n",
    "    def reward_compute(self):\n",
    "        penalty = calculate_penalty(self.water_level)\n",
    "        return -penalty\n",
    "\n",
    "        # TODO 0 weghalen\n",
    "\n",
    "    def rain(self):\n",
    "        return uniform(0.0,0.25)\n",
    "\n",
    "    # back-up\n",
    "    def terminal(self):\n",
    "        if self.water_level > 1.0 or self.water_level < 0.0:\n",
    "            return True \n",
    "        return False\n",
    "\n",
    "    def execute(self, actions):\n",
    "        ## Check the action is either 0 or 1 -- pump on or off.\n",
    "        assert actions == 0 or actions == 1\n",
    "\n",
    "        ## Increment timestamp\n",
    "        self.timestep += 1\n",
    "        \n",
    "        ## Update the current_temp\n",
    "        self.water_level = self.response(actions)\n",
    "        self.water_level += self.rain()\n",
    "\n",
    "        ## Compute the reward\n",
    "        reward = self.reward_compute()[0]\n",
    "\n",
    "        ## The only way to go terminal is to exceed max_episode_timestamp.\n",
    "        ## terminal == False means episode is not done\n",
    "        ## terminal == True means it is done.\n",
    "        terminal = self.terminal()\n",
    "        \n",
    "        return self.water_level, terminal, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFgYnj8tXMuA"
   },
   "source": [
    "Create the environment and agent (just run these cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phLJ7ztWXMuB"
   },
   "outputs": [],
   "source": [
    "environment = environment = Environment.create(\n",
    "    environment=PolderEnvironment,\n",
    "    max_episode_timesteps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYp1dfzxXMuB"
   },
   "outputs": [],
   "source": [
    "agent = Agent.create(\n",
    "    agent='tensorforce', environment=environment, update=64,\n",
    "    optimizer=dict(optimizer='adam', learning_rate=1e-3),\n",
    "    objective='policy_gradient', reward_estimation=dict(horizon=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual training, so it can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTt3AVA8XMuC"
   },
   "outputs": [],
   "source": [
    "for _ in range(200):\n",
    "    states = environment.reset()\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        actions = agent.act(states=states)\n",
    "        states, terminal, reward = environment.execute(actions=actions)\n",
    "        agent.observe(terminal=terminal, reward=reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0-zP4JqXMuC"
   },
   "source": [
    "Run this cell to see whether it worked and see the results. If it didn't work, you'll have to change you penalty function and you'll have to rerun all the cells from the changed cell onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "ojB_FZjHXMuD",
    "outputId": "3148f3b7-8fd4-44df-81b7-44947bb29f26"
   },
   "outputs": [],
   "source": [
    "### Initialize\n",
    "environment.reset()\n",
    "environment.water_level = np.random.uniform(low=0.0, high=1.0, size=(1,))\n",
    "states = environment.water_level\n",
    "\n",
    "internals = agent.initial_internals()\n",
    "terminal = False\n",
    "\n",
    "### Run an episode\n",
    "temp = [environment.water_level[0]]\n",
    "while not terminal:\n",
    "    actions, internals = agent.act(states=states, internals=internals, independent=True)\n",
    "    states, terminal, reward = environment.execute(actions=actions)\n",
    "    temp += [states[0]]\n",
    "\n",
    "fig = px.line(df, x=df.index, y='water level', title='Reinforcement learning poldergemaal: water level over time',\n",
    "             labels={\n",
    "                     \"index\": \"Time\"})\n",
    "fig.update_yaxes(range = [0,1])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "poldergemaal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
